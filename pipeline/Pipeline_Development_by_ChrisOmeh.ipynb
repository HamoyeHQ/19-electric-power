{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "UNCOMMENT BELOW CODES AND RUN\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python -m pip install --user --upgrade pip\n",
    "#!pip3 install pandas==0.23.4 matplotlib==3.0.3 scipy==1.2.1 scikit-learn==0.22 tensorflow==2.3 keras==2.4.3 --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install kfp --upgrade --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install statsmodels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline Archetecture\n",
    "## Components\n",
    "1. Data Ingestion\n",
    "2. Data Wrangling\n",
    "3. Data Cleaning\n",
    "4. Data Preprocessing\n",
    "5. Data Modeling\n",
    "6. Model Deploy\n",
    "6. Model Servicing\n",
    "6. Evaluation metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kubeflow Output Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jovyan\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/home/jovyan/stage-f-05-electric-power/data/out\"\n",
    "#input_path= \"https://storage.cloud.google.com/chrisc-bucket/household_power_consumption.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Ingestion Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_ingestion(input_path, output_path):\n",
    "    import subprocess\n",
    "    import pickle\n",
    "    import sys\n",
    "    \n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pandas\", \"scikit-learn\"])\n",
    "\n",
    "    import pandas as pd\n",
    "    pd.set_option('display.max_columns', None)\n",
    "    pd.set_option('display.max_row', None)\n",
    "    pd.set_option('display.max_colwidth', 100)\n",
    "    pd.set_option('display.float_format', lambda x: '%.2f' % x)\n",
    "    \n",
    "    \n",
    "    uci_path = \"http://archive.ics.uci.edu/ml/machine-learning-databases/00235/household_power_consumption.zip\"\n",
    "    download_url = input_path if input_path else uci_path\n",
    "    \n",
    "    subprocess.run([\"wget\", \"-O\", \"household_power_consumption.zip\", download_url])\n",
    "    subprocess.run([\"unzip\", \"household_power_consumption.zip\"])\n",
    "    subprocess.run([\"unzip\", \"*.zip\"])\n",
    "    \n",
    "    print(\"======Successfully Unzipped the dataset file======\")\n",
    "    \n",
    "    subprocess.call([\"rm\", \"-r\", \"*.zip\"])\n",
    "    data_load = pd.read_csv(\"household_power_consumption.txt\", delimiter=';', parse_dates = {\"datetime\":[0,1]},\n",
    "                 low_memory = False, infer_datetime_format=True)\n",
    "    \n",
    "    print(\"======Successfully loaded the dataset file======\")\n",
    "    \n",
    "    with open(f\"{output_path}/datai.pkl\", \"wb\") as data:\n",
    "        pickle.dump(data_load, data)\n",
    "    \n",
    "    \n",
    "    print(\"=====Data Ingestion Finished Successfully=======\")\n",
    "    return data_load.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======Successfully Unzipped the dataset file======\n",
      "======Successfully loaded the dataset file======\n",
      "=====Data Ingestion Finished Successfully=======\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2075259, 8)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_ingestion(\"\",output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_wrangling(output_path):\n",
    "    import subprocess, pickle, sys\n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pandas\", \"numpy\", \"scikit-learn\"])\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    with open(f\"{output_path}/datai.pkl\", 'rb') as f:\n",
    "        data = pickle.load(f)\n",
    "    print(data.columns)\n",
    "        \n",
    "    print(\"=====Renaming the dataset columns=====\")   \n",
    "    \n",
    "    cols = {'Global_active_power': 'Global_active_power(KW)',\n",
    "            'Global_reactive_power':'Global_reactive_power(KW)',\n",
    "            'Voltage': 'Voltage(V)',\n",
    "            'Global_intensity': 'Global_intensity(Ampere)',\n",
    "            'Sub_metering_1': 'Sub_metering_1(WH)',\n",
    "            'Sub_metering_2': 'Sub_metering_2(WH)',\n",
    "            'Sub_metering_3': 'Sub_metering_3(WH)'\n",
    "       }\n",
    "    data.rename(cols, axis = 1, inplace = True)\n",
    "    print(\"=====Dataset columns rename successfully=====\",\"\\n\")\n",
    "    \n",
    "    print(\"....................................................................\")\n",
    "    print(\"=====Setting the datetime column to pandas datetime=====\")\n",
    "    print(\"....................................................................\")\n",
    "    \n",
    "    data[\"datetime\"]  = pd.to_datetime(data[\"datetime\"])\n",
    "    data[\"Year\"] = data[\"datetime\"].apply(lambda X:X.year)\n",
    "    data[\"Quarter\"] = data[\"datetime\"].apply(lambda X:X.quarter)\n",
    "    data[\"Month\"]  = data[\"datetime\"].apply(lambda X:X.month)\n",
    "    data['Month_name'] = data[\"datetime\"].apply(lambda X:X.month_name())\n",
    "    data[\"dayofweek\"] = data[\"datetime\"].apply(lambda X:X.weekday())\n",
    "    data['day_name'] = data['datetime'].apply(lambda X:X.day_name())\n",
    "    data = data.set_index(\"datetime\")\n",
    "    \n",
    "    print(\"=====Datetime column successfully converted to pandas datetime=====\")\n",
    "    print(\"................................................................... \",\"\\n\")\n",
    "    \n",
    "    print(f\"The number of rows and columns is now {data.shape[0]} and {data.shape[1]} respectively\",\"\\n\")\n",
    "    \n",
    "    print(\"===Unique values in Year, Quarter, Month, Month_name, dayofweek and day_name columns===\")\n",
    "    print(\"Year column unique values:\", data[\"Year\"].unique())\n",
    "    print(\"Quater column unique values:\",data[\"Quarter\"].unique())\n",
    "    print(\"Month column unique values:\", data[\"Month\"].unique())\n",
    "    print(\"Month_name column unique values:\", data['Month_name'].unique())\n",
    "    print(\"dayofweek column unique values:\", data['dayofweek'].unique())\n",
    "    print(\"day_name column unique values:\", data['day_name'].unique())\n",
    "    \n",
    "    with open(f\"{output_path}/dataw.pkl\", \"wb\") as wrangled_data:\n",
    "        pickle.dump(data,wrangled_data)\n",
    "    \n",
    "    print(\"=====Data Wrangling Finished Successfully=======\")\n",
    "    return data.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['datetime', 'Global_active_power', 'Global_reactive_power', 'Voltage',\n",
      "       'Global_intensity', 'Sub_metering_1', 'Sub_metering_2',\n",
      "       'Sub_metering_3'],\n",
      "      dtype='object')\n",
      "=====Renaming the dataset columns=====\n",
      "=====Dataset columns rename successfully===== \n",
      "\n",
      "....................................................................\n",
      "=====Setting the datetime column to pandas datetime=====\n",
      "....................................................................\n",
      "=====Datetime column successfully converted to pandas datetime=====\n",
      "...................................................................  \n",
      "\n",
      "The number of rows and columns is now 2075259 and 13 respectively \n",
      "\n",
      "===Unique values in Year, Quarter, Month, Month_name, dayofweek and day_name columns===\n",
      "Year column unique values: [2006 2007 2008 2009 2010]\n",
      "Quater column unique values: [4 1 2 3]\n",
      "Month column unique values: [12  1  2  3  4  5  6  7  8  9 10 11]\n",
      "Month_name column unique values: ['December' 'January' 'February' 'March' 'April' 'May' 'June' 'July'\n",
      " 'August' 'September' 'October' 'November']\n",
      "dayofweek column unique values: [5 6 0 1 2 3 4]\n",
      "day_name column unique values: ['Saturday' 'Sunday' 'Monday' 'Tuesday' 'Wednesday' 'Thursday' 'Friday']\n",
      "=====Data Wrangling Finished Successfully=======\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2075259, 13)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_wrangling(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "1. Handling Null entries\n",
    "2. Checking and droping duplicate entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_cleaning(output_path):\n",
    "    import subprocess, sys, pickle\n",
    "    \n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pandas\", \"scikit-learn\"])\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    \n",
    "    with open(f\"{output_path}/dataw.pkl\", \"rb\") as dataset:\n",
    "        data = pickle.load(dataset)\n",
    "    \n",
    "    #Replace ? with np.nan\n",
    "    data.replace('?', np.nan, inplace = True)\n",
    "    colums = ['Global_active_power(KW)', 'Global_reactive_power(KW)', 'Voltage(V)',\n",
    "              'Global_intensity(Ampere)','Sub_metering_1(WH)', 'Sub_metering_2(WH)', 'Sub_metering_3(WH)']\n",
    "    for cols in colums:\n",
    "        print('==========',cols,'==========')\n",
    "        data[cols].fillna(value = 1, inplace = True)\n",
    "    pass\n",
    "    \n",
    "    print(\"=====Checking for Duplicated Entries=====\",\"\\n\")\n",
    "    print(f'Duplicate check shows there are some dupliacte entries with shape: {data[data.duplicated()].shape}')\n",
    "    \n",
    "    print(\"=====Dropping Duplicated Entries=====\",\"\\n\")\n",
    "    \n",
    "    data = data.drop_duplicates()\n",
    "    print(\"=====Duplicated Entries Successfully Dropped=====\")\n",
    "    \n",
    "    with open(f\"{output_path}/datac.pkl\", \"wb\") as cleaned_data:\n",
    "        pickle.dump(data, cleaned_data)\n",
    "    \n",
    "    print(\"=====Data Cleaning Finished Successfully=======\")\n",
    "    return data.shape\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== Global_active_power(KW) ==========\n",
      "========== Global_reactive_power(KW) ==========\n",
      "========== Voltage(V) ==========\n",
      "========== Global_intensity(Ampere) ==========\n",
      "========== Sub_metering_1(WH) ==========\n",
      "========== Sub_metering_2(WH) ==========\n",
      "========== Sub_metering_3(WH) ==========\n",
      "=====Checking for Duplicated Entries===== \n",
      "\n",
      "Duplicate check shows there are some dupliacte entries with shape: (56666, 13)\n",
      "=====Dropping Duplicated Entries===== \n",
      "\n",
      "=====Duplicated Entries Successfully Dropped=====\n",
      "=====Data Cleaning Finished Successfully=======\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2018593, 13)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_cleaning(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA and Stationarity Check\n",
    "1. EDA\n",
    "2. Visualization\n",
    "3. ADF Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stationarity_check(output_path):\n",
    "    import sys, pickle, subprocess\n",
    "    \n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pandas\", \"matplotlib\", \"scikit-learn\"])\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    #import seaborn as sns\n",
    "    plt.style.use('ggplot')\n",
    "    plt.rcParams['font.size'] = 10\n",
    "    \n",
    "    #Time series libraries\n",
    "    from scipy import stats\n",
    "    from statsmodels.tsa.stattools import adfuller, acf, pacf\n",
    "    from statsmodels.graphics.tsaplots import pacf, plot_acf,plot_pacf\n",
    "    from pandas.plotting import autocorrelation_plot\n",
    "    from statsmodels.tsa.arima_model import  ARIMA\n",
    "    import statsmodels.api as sm\n",
    "    \n",
    "    with open(f\"{output_path}/datac.pkl\", \"rb\") as dataset:\n",
    "        data = pickle.load(dataset)\n",
    "        \n",
    "    data_col = data['Global_active_power(KW)']\n",
    "    data_col = data_col.astype('float')\n",
    "    original_timeseries = data_col\n",
    "    rolling_mean = data_col.rolling(window = 30).mean()\n",
    "    rolling_var =  data_col.rolling(window = 30).var()\n",
    "    rolling_std =  data_col.rolling(window = 30).std()\n",
    "\n",
    "    print(\"==========Plotting of Window Rollings Plots==========\",'\\n')\n",
    "#     plt.figure(figsize = (15,10))\n",
    "#     sns.despine(left = True)\n",
    "#     original_plot = plt.plot(original_timeseries, label = \"Original Timeseries\", color = 'purple', marker = 'o')\n",
    "#     rolling_mean_plot = plt.plot(rolling_mean, color = 'red', label = \"rolling_mean\", marker = 'x')\n",
    "#     rolling_var_plot = plt.plot(rolling_var,color = 'green', label = 'rolling variance', marker = '^')\n",
    "#     rolling_std_plot = plt.plot(rolling_std, label = \"rolling_std\", color = 'cyan',marker = '*' )\n",
    "  \n",
    "#     plt.legend(loc='best'); plt.title('Rolling of Mean & Standard Deviation')\n",
    "#     plt.show()\n",
    "\n",
    "    print('\\n',\"==========Augument Dickey Fuller Test==========\")\n",
    "    print(\"===The result show the TS is stationary because the ADF TEST STATISTICS value is lower than the Critical values===\")\n",
    "    adf_result = adfuller(original_timeseries.values, autolag = 'AIC')\n",
    "    adf_output = pd.Series(adf_result[0:4], index= ['ADF Statistic','p-value','#Lags Used','Total used observations'])\n",
    "\n",
    "    for key, value in adf_result[4].items():\n",
    "        adf_output['Critical Value (%s)'%key] = value\n",
    "    print(adf_output)\n",
    "    \n",
    "    with open(f\"{output_path}/datats.pkl\", \"wb\") as eda_dataset:\n",
    "        pickle.dump(data, eda_dataset)\n",
    "        \n",
    "    print(\"=====Stationarity check Finished Successfully=======\")    \n",
    "    return data.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========Plotting of Window Rollings Plots========== \n",
      "\n",
      "\n",
      " ==========Augument Dickey Fuller Test==========\n",
      "===The result show the TS is stationary because the ADF TEST STATISTICS value is lower than the Critical values===\n",
      "=====Stationarity check Finished Successfully=======\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2018593, 13)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stationarity_check(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "1. Feature Engineering\n",
    "2. Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_preprocessing(output_path):\n",
    "    import subprocess, sys, pickle\n",
    "    \n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pandas\", \"matplotlib\", \"scikit-learn\"])\n",
    "    \n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "    from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, LabelEncoder\n",
    "    plt.style.use('ggplot')\n",
    "    plt.rcParams['font.size'] = 10\n",
    "    \n",
    "    with open(f\"{output_path}/datats.pkl\", \"rb\") as dataset:\n",
    "        data = pickle.load(dataset)\n",
    "        \n",
    "    \"\"\"\n",
    "    Since we are using a Recurrent Neural network, it is better to use MinMaxScaler\n",
    "    \"\"\"\n",
    "    data = data['Global_active_power(KW)'].values.reshape(-1,1)\n",
    "    scaler = MinMaxScaler(feature_range = (0,1))\n",
    "    \n",
    "    # The fit_transform method converts the data to numpy array stored in dataset variable\n",
    "    dataset = scaler.fit_transform(data) \n",
    "    print(\"The type of dataset is {}\". format(type(dataset)))\n",
    "    print(\"============================Scaled dataset====================\",'\\n')\n",
    "    print(dataset[:5])\n",
    "    print(\"============================Scaled dataset====================\",'\\n')\n",
    "    \n",
    "    with open(f\"{output_path}/datapr.pkl\", \"wb\") as processed_data:\n",
    "        pickle.dump(dataset, processed_data)\n",
    "    \n",
    "    print(\"=====Preprocessing Finished Successfully=======\")    \n",
    "    return pd.DataFrame(dataset).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The type of dataset is <class 'numpy.ndarray'>\n",
      "============================Scaled dataset==================== \n",
      "\n",
      "[[0.37479631]\n",
      " [0.47836321]\n",
      " [0.47963064]\n",
      " [0.48089806]\n",
      " [0.32500453]]\n",
      "============================Scaled dataset==================== \n",
      "\n",
      "=====Preprocessing Finished Successfully=======\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(2018593, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_preprocessing(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_modelling(output_path):\n",
    "    import subprocess, sys, pickle\n",
    "    \n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pandas\", \"scikit-learn\",\"tensorflow==2.3\", \"keras==2.4.3\"])\n",
    "    \n",
    "    #Modelling libaries\n",
    "    import numpy as np\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    import keras\n",
    "    from keras.models import Sequential\n",
    "    from keras.layers import Dropout, Dense, LSTM\n",
    "    from keras.callbacks import EarlyStopping\n",
    "    \n",
    "    with open(f\"{output_path}/datapr.pkl\", \"rb\") as data:\n",
    "        dataset = pickle.load(data)\n",
    "        \n",
    "    train_size = int(len(dataset)*0.8)\n",
    "    test_size = int(len(dataset)*0.2)\n",
    "\n",
    "    train, test = dataset[:train_size,:], dataset[train_size:len(dataset),:]\n",
    "\n",
    "\n",
    "    print(\"=====Converting an array of values into a dataset matrix done successfully=====\",'\\n')\n",
    "    def create_dataset(dataset, look_back):\n",
    "        X, Y = [], []\n",
    "        for i in range(len(dataset)-look_back-1):\n",
    "            x1 = dataset[i:(i+look_back), 0]\n",
    "            X.append(x1)\n",
    "            Y.append(dataset[i + look_back, 0])\n",
    "        return np.array(X), np.array(Y)\n",
    "    \n",
    "    X_train, y_train = create_dataset(train, 30)\n",
    "    X_test, y_test = create_dataset(test,30)\n",
    "\n",
    "    print(\"====Reshaping input to be [samples, no_features, time steps] done successfully====\",'\\n')\n",
    "    X_train = X_train.reshape(X_train.shape[0],1,X_train.shape[1])\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, X_test.shape[1])\n",
    "\n",
    "    print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(100, input_shape=(X_train.shape[1], X_train.shape[2])))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "    history = model.fit(X_train, y_train, epochs=5, batch_size=70, validation_data=(X_test, y_test), \n",
    "                    callbacks=[EarlyStopping(monitor='val_loss', patience=10)], verbose=1, shuffle=False)\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    #Save the model to the designated \n",
    "    model.save(f'{output_path}/lstm_model.h5')\n",
    "    \n",
    "    with open(f\"{output_path}/train_data\", \"wb\") as training_data:\n",
    "        pickle.dump((X_train,y_train), training_data)\n",
    "    \n",
    "    with open(f\"{output_path}/test_data\", \"wb\") as testing_data:\n",
    "        pickle.dump((X_test,y_test), testing_data)\n",
    "        \n",
    "    return \"=====Data Modelling Finished Successfully=======\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Converting an array of values into a dataset matrix done successfully===== \n",
      "\n",
      "====Reshaping input to be [samples, no_features, time steps] done successfully==== \n",
      "\n",
      "(1614843, 1, 30) (403688, 1, 30) (1614843,) (403688,)\n",
      "Epoch 1/5\n",
      "23070/23070 [==============================] - 96s 4ms/step - loss: 7.7852e-04 - val_loss: 4.2301e-04\n",
      "Epoch 2/5\n",
      "23070/23070 [==============================] - 93s 4ms/step - loss: 6.7475e-04 - val_loss: 4.0997e-04\n",
      "Epoch 3/5\n",
      "23070/23070 [==============================] - 94s 4ms/step - loss: 6.6549e-04 - val_loss: 4.1442e-04\n",
      "Epoch 4/5\n",
      "23070/23070 [==============================] - 90s 4ms/step - loss: 6.5828e-04 - val_loss: 4.1312e-04\n",
      "Epoch 5/5\n",
      "23070/23070 [==============================] - 89s 4ms/step - loss: 6.5341e-04 - val_loss: 4.1422e-04\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (None, 100)               52400     \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 101       \n",
      "=================================================================\n",
      "Total params: 52,501\n",
      "Trainable params: 52,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'=====Data Modelling Finished Successfully======='"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_modelling(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction/Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forecasting(output_path):\n",
    "    import sys, subprocess, pickle\n",
    "    \n",
    "    subprocess.run([sys.executable, \"-m\", \"pip\", \"install\", \"pandas\", \"scikit-learn\", \"matplotlib\",\"tensorflow==2.3\", \"keras==2.4.3\"])\n",
    "    \n",
    "    from tensorflow import keras\n",
    "    import keras\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    from sklearn.metrics import accuracy_score, mean_squared_error\n",
    "    from sklearn.preprocessing import StandardScaler, RobustScaler, MinMaxScaler, LabelEncoder\n",
    "    scaler = MinMaxScaler(feature_range = (0,1))\n",
    "        \n",
    "    with open(f\"{output_path}/train_data\", \"rb\") as train:\n",
    "        train_data = pickle.load(train)\n",
    "        \n",
    "    X_train, y_train = train_data\n",
    "    \n",
    "    with open(f\"{output_path}/test_data\", \"rb\") as test:\n",
    "        test_data = pickle.load(test)\n",
    "        \n",
    "    X_test, y_test = test_data\n",
    "    \n",
    "    model = keras.models.load_model(f'{output_path}/lstm_model.h5')\n",
    "    train_prediction = model.predict(X_train)\n",
    "    test_prediction = model.predict(X_test)\n",
    "    \n",
    "    print(train_prediction[:5])\n",
    "    print(test_prediction[:5])\n",
    "\n",
    "    print(\"**********Mean Square Error and RMSE**********\")\n",
    "    print()\n",
    "    print(\"===========================The Train Mean Suared Error is:=================================\",\"\\n\")\n",
    "    print(round(mean_squared_error(y_train, train_prediction),3),\"\\n\")\n",
    "    print(\"===========================The Train Root Mean Suared Error is:=================================\",\"\\n\")\n",
    "    print(round(np.sqrt(mean_squared_error(y_train, train_prediction)),3),\"n\")\n",
    "    print(\"===========================The Test Mean Suared Error is:=================================\",\"\\n\")\n",
    "    print(round(mean_squared_error(y_test, test_prediction),3),\"\\n\")\n",
    "    print(\"===========================The Test Root Mean Suared Error is:=================================\",\"\\n\")\n",
    "    print(round(np.sqrt(mean_squared_error(y_test, test_prediction)),3))\n",
    "    \n",
    "    \n",
    "    print(\"**********comparing the actual and predictions for the last 200 minutes**********\")\n",
    "#     data_list = [x for x in range(200)]\n",
    "#     plt.figure(figsize=(8,4))\n",
    "#     plt.plot(data_list, y_test[0][:200], marker='*', label=\"actual\")\n",
    "#     plt.plot(data_list, test_prediction[:,0][:200], '^', label=\"prediction\")\n",
    "#     plt.tight_layout()\n",
    "#     sns.despine(top=True)\n",
    "#     plt.subplots_adjust(left=0.07)\n",
    "#     plt.ylabel('Global_active_power(KW)', size=15)\n",
    "#     plt.xlabel('Time step', size=15)\n",
    "#     plt.legend(fontsize=15)\n",
    "#     plt.show();\n",
    "    \n",
    "    with open(f\"{output_path}/predict.pkl\", \"wb\") as prediction:\n",
    "        pickle.dump(test_prediction, prediction)\n",
    "        \n",
    "    return \"======Forecasting Finished Successfully=====\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.2744965 ]\n",
      " [0.24158505]\n",
      " [0.3221066 ]\n",
      " [0.36155543]\n",
      " [0.37524563]]\n",
      "[[0.2502526 ]\n",
      " [0.25677687]\n",
      " [0.25488603]\n",
      " [0.2554449 ]\n",
      " [0.25003466]]\n",
      "=Since we transformed our train and test data, we need to inverse it to get the real predicted values=\n",
      "**********Mean Square Error and RMSE**********\n",
      "\n",
      "===========================The Train Mean Suared Error is:================================= \n",
      "\n",
      "0.001 \n",
      "\n",
      "===========================The Train Root Mean Suared Error is:================================= \n",
      "\n",
      "0.025 n\n",
      "===========================The Test Mean Suared Error is:================================= \n",
      "\n",
      "0.0 \n",
      "\n",
      "===========================The Test Root Mean Suared Error is:================================= \n",
      "\n",
      "0.02\n",
      "**********comparing the actual and predictions for the last 200 minutes**********\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'======Forecasting Finished Successfully====='"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forecasting(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def model_deploy(output_pat):\n",
    "#     import subprocess, sys, pickle\n",
    "    \n",
    "#     subprocess([sys.executable, \"-m\", \"pip\", \"install\", \"pandas\", \"scikit-learn\"])\n",
    "    \n",
    "#     import pandas as pd\n",
    "#     import numpy as np\n",
    "    \n",
    "    \n",
    "#     ai_platform_deploy_operation = comp.load_component_from_url(\n",
    "#     \"https://storage.googleapis.com/{}/energy_bucket/deploy/model.pkl\".format(BUCKET))\n",
    "#     help(ai_platform_deploy_operation)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Components Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kfp\n",
    "from kfp import dsl\n",
    "import kfp.components as comp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "!which dsl-compile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_download_comp = comp.func_to_container_op(data_ingestion, base_image= \"python:3.7\")\n",
    "\n",
    "data_wrangled_comp = comp.func_to_container_op(data_wrangling, base_image=\"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "\n",
    "data_cleaning_comp = comp.func_to_container_op(data_cleaning, base_image=\"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "\n",
    "timeseries_check_comp =  comp.func_to_container_op(stationarity_check,base_image=\"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "\n",
    "data_preprocess_comp = comp.func_to_container_op(data_preprocessing, base_image=\"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "\n",
    "data_modelling_comp = comp.func_to_container_op(data_modelling, base_image=\"tensorflow/tensorflow:latest-gpu-py3\")\n",
    "\n",
    "forecasting_comp = comp.func_to_container_op(forecasting, base_image=\"tensorflow/tensorflow:latest-gpu-py3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dsl.pipeline(name =\"Electric Power Consumption\",\n",
    "        description = \"A ML Pipeline that load, clean, preprocess, train and forecast Electric Power Consumption\")\n",
    "\n",
    "def electric_power_consumption(input_path:str,\n",
    "                               output_path:str):\n",
    "    \n",
    "    volume_op = dsl.VolumeOp(\n",
    "        name=\"data_volume\",\n",
    "        resource_name=\"data-volume\",\n",
    "        size=\"3Gi\",\n",
    "        modes=dsl.VOLUME_MODE_RWO)\n",
    "        \n",
    "    download_container = data_download_comp(input_path, output_path)\\\n",
    "                                        .add_pvolumes({output_path: volume_op.volume})\n",
    "    data_wrangled_container = data_wrangled_comp(output_path)\\\n",
    "                                        .add_pvolumes({output_path:download_container.pvolume})\n",
    "    data_clean_container = data_cleaning_comp(output_path)\\\n",
    "                                        .add_pvolumes({output_path:data_wrangled_container.pvolume})\n",
    "    timeseries_check_container = timeseries_check_comp(output_path)\\\n",
    "                                        .add_pvolumes({output_path:data_clean_container.pvolume})\n",
    "    data_preprocess_container = data_preprocess_comp(output_path)\\\n",
    "                                        .add_pvolumes({output_path: timeseries_check_container.pvolume})\n",
    "    data_modelling_container = data_modelling_comp(output_path)\\\n",
    "                                        .add_pvolumes({output_path:data_preprocess_container.pvolume})\n",
    "    forecasting_conatiner = forecasting_comp(output_path)\\\n",
    "                                        .add_pvolumes({output_path:data_modelling_container.pvolume})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_PATH = '/mnt'\n",
    "\n",
    "INPUT_PATH = \"\"\n",
    "\n",
    "MODEL_PATH = 'lstm_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_func = electric_power_consumption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = kfp.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/experiments/details/57e48fb3-ad48-416e-b2a7-efe1fca171d9\" target=\"_blank\" >Experiment details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<a href=\"/pipeline/#/runs/details/874a3d5b-cb8d-4df5-bd57-e5f461f4ebff\" target=\"_blank\" >Run details</a>."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "experiment_name = 'Electric_Power_Consumption_Pipeline'\n",
    "run_name = f'{pipeline_func.__name__} run'\n",
    "\n",
    "arguments = {\"input_path\": INPUT_PATH,\n",
    "             \"output_path\":OUTPUT_PATH,\n",
    "             \"output_path\":MODEL_PATH\n",
    "             }\n",
    "\n",
    "# Compile pipeline to generate compressed YAML definition of the pipeline.\n",
    "kfp.compiler.Compiler().compile(pipeline_func, f'{experiment_name}.zip')\n",
    "\n",
    "# Submit pipeline directly from pipeline function\n",
    "run_result = client.create_run_from_pipeline_func(pipeline_func, \n",
    "                                                  experiment_name=experiment_name, \n",
    "                                                  run_name=run_name, \n",
    "                                                  arguments=arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
